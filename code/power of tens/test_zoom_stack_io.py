#!/usr/bin/env python3
"""
ç¼©æ”¾æ ˆè¾“å…¥è¾“å‡ºæµ‹è¯•è„šæœ¬
å±•ç¤º ZoomStack çš„å…·ä½“è¾“å…¥è¾“å‡ºæ ¼å¼
"""

import torch
import numpy as np

def test_zoom_stack_io():
    """æµ‹è¯•ç¼©æ”¾æ ˆçš„è¾“å…¥è¾“å‡º"""
    
    print("=== ç¼©æ”¾æ ˆè¾“å…¥è¾“å‡ºæµ‹è¯• ===\n")
    
    # ==================== è¾“å…¥å‚æ•° ====================
    print("ğŸ“¥ è¾“å…¥å‚æ•°:")
    zoom_factors = [1, 2, 4, 8]
    H, W = 256, 256  # ä½¿ç”¨è¾ƒå°åˆ†è¾¨ç‡ä¾¿äºæ¼”ç¤º
    device = "cpu"   # ä½¿ç”¨ CPU ä¾¿äºæ¼”ç¤º
    
    print(f"  - zoom_factors: {zoom_factors}")
    print(f"  - å›¾åƒå°ºå¯¸: {H}Ã—{W}")
    print(f"  - è®¾å¤‡: {device}")
    print(f"  - å±‚æ•°: {len(zoom_factors)}")
    
    # ==================== åˆ›å»ºç¼©æ”¾æ ˆ ====================
    print(f"\nğŸ”„ åˆ›å»ºç¼©æ”¾æ ˆ...")
    
    # ç®€åŒ–ç‰ˆç¼©æ”¾æ ˆï¼ˆä¸ä¾èµ–å¤–éƒ¨åº“ï¼‰
    class SimpleZoomStack:
        def __init__(self, zoom_factors, H, W, device="cpu"):
            self.zoom_factors = zoom_factors
            self.N = len(zoom_factors)
            self.H, self.W = H, W
            self.device = device
            
            # åˆå§‹åŒ–å±‚ - ä»é«˜æ–¯å™ªå£°å¼€å§‹
            self.layers = []
            for i, zoom in enumerate(zoom_factors):
                # æ¯å±‚éƒ½æ˜¯ HÃ—WÃ—3 å¼ é‡
                layer = torch.randn(H, W, 3) * 0.1  # å°å¹…åº¦å™ªå£°
                self.layers.append(layer)
    
    zoom_stack = SimpleZoomStack(zoom_factors, H, W, device)
    
    # ==================== è¾“å‡ºåˆ†æ ====================
    print(f"\nğŸ“¤ è¾“å‡ºç»“æ„:")
    print(f"  - ç¼©æ”¾æ ˆå¯¹è±¡: {type(zoom_stack)}")
    print(f"  - æ€»å±‚æ•°: {zoom_stack.N}")
    print(f"  - å±‚åˆ—è¡¨é•¿åº¦: {len(zoom_stack.layers)}")
    
    print(f"\nğŸ“Š æ¯å±‚è¯¦ç»†ä¿¡æ¯:")
    for i, (layer, zoom) in enumerate(zip(zoom_stack.layers, zoom_factors)):
        print(f"  L_{i}: å½¢çŠ¶={layer.shape}, dtype={layer.dtype}, ç¼©æ”¾={zoom}x")
        print(f"       æ•°æ®èŒƒå›´=[{layer.min():.3f}, {layer.max():.3f}]")
        print(f"       å†…å­˜ä½¿ç”¨={layer.numel() * 4 / 1024:.1f} KB")  # float32 = 4 bytes
    
    # ==================== å®é™…æ•°æ®ç¤ºä¾‹ ====================
    print(f"\nğŸ” æ•°æ®ç¤ºä¾‹ï¼ˆå·¦ä¸Šè§’ 3Ã—3 åƒç´ ï¼‰:")
    for i, layer in enumerate(zoom_stack.layers):
        print(f"\n  L_{i} (ç¼©æ”¾={zoom_factors[i]}x):")
        sample = layer[:3, :3, 0]  # åªçœ‹çº¢è‰²é€šé“çš„ 3Ã—3 åŒºåŸŸ
        print(f"    {sample.numpy()}")
    
    # ==================== ä½¿ç”¨åœºæ™¯ ====================
    print(f"\nğŸ¯ å…¸å‹ä½¿ç”¨åœºæ™¯:")
    
    # 1. è·å–å•å±‚
    layer_0 = zoom_stack.layers[0]
    print(f"  1. è·å–ç¬¬0å±‚: å½¢çŠ¶={layer_0.shape}")
    
    # 2. ä¿®æ”¹å•å±‚
    zoom_stack.layers[1] = torch.zeros_like(zoom_stack.layers[1])
    print(f"  2. ä¿®æ”¹ç¬¬1å±‚ä¸ºé›¶: æ–°èŒƒå›´=[{zoom_stack.layers[1].min():.3f}, {zoom_stack.layers[1].max():.3f}]")
    
    # 3. æ‰¹é‡å¤„ç†
    all_layers = zoom_stack.layers
    total_memory = sum(layer.numel() * 4 for layer in all_layers) / (1024*1024)
    print(f"  3. æ€»å†…å­˜ä½¿ç”¨: {total_memory:.2f} MB")
    
    # ==================== ä¸è®ºæ–‡å¯¹åº”å…³ç³» ====================
    print(f"\nğŸ“š ä¸è®ºæ–‡çš„å¯¹åº”å…³ç³»:")
    print(f"  - è®ºæ–‡ç¬¦å· L = [L_0, L_1, ..., L_{{N-1}}]")
    print(f"  - å®é™…å®ç°: zoom_stack.layers = {[f'L_{i}' for i in range(zoom_stack.N)]}")
    print(f"  - ç¼©æ”¾å› å­ p_i = 2^i: {[f'p_{i}=2^{i}={2**i}' for i in range(zoom_stack.N)]}")
    
    # ==================== æ•°æ®æµç¤ºä¾‹ ====================
    print(f"\nğŸ”„ æ•°æ®æµç¤ºä¾‹:")
    print(f"  è¾“å…¥: zoom_factors={zoom_factors}")
    print(f"  å¤„ç†: åˆ›å»º {zoom_stack.N} ä¸ª {H}Ã—{W}Ã—3 å¼ é‡")
    print(f"  è¾“å‡º: ç¼©æ”¾æ ˆå¯¹è±¡ï¼ŒåŒ…å« {len(zoom_stack.layers)} å±‚")
    
    return zoom_stack

def demonstrate_zoom_stack_operations():
    """æ¼”ç¤ºç¼©æ”¾æ ˆçš„åŸºæœ¬æ“ä½œ"""
    print(f"\n\n=== ç¼©æ”¾æ ˆæ“ä½œæ¼”ç¤º ===")
    
    zoom_stack = test_zoom_stack_io()
    
    print(f"\nğŸ› ï¸ åŸºæœ¬æ“ä½œ:")
    
    # è¯»å–æ“ä½œ
    print(f"  - è¯»å–: layer = zoom_stack.layers[i]")
    example_layer = zoom_stack.layers[0]
    print(f"    ç¤ºä¾‹: layer.shape = {example_layer.shape}")
    
    # å†™å…¥æ“ä½œ  
    print(f"  - å†™å…¥: zoom_stack.layers[i] = new_layer")
    new_layer = torch.ones_like(example_layer) * 0.5
    zoom_stack.layers[0] = new_layer
    print(f"    ç¤ºä¾‹: è®¾ç½®ä¸ºå¸¸æ•° 0.5ï¼ŒèŒƒå›´=[{zoom_stack.layers[0].min():.1f}, {zoom_stack.layers[0].max():.1f}]")
    
    # æ‰¹é‡æ“ä½œ
    print(f"  - æ‰¹é‡: all_layers = zoom_stack.layers")
    all_shapes = [layer.shape for layer in zoom_stack.layers]
    print(f"    ç¤ºä¾‹: æ‰€æœ‰å½¢çŠ¶ = {all_shapes}")

if __name__ == "__main__":
    demonstrate_zoom_stack_operations() 